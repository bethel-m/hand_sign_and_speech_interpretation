# Hand sign and Speech Interpretation

## Summary
This project aims to interprete hand signs by deaf people to Local Nigerian languages, and vice versa from speech to hand signs.

## Background

Looking in our society today... there a lot of people with special needs or select disabilities, and one of the most common one is the inability to speak, and some times the person who has this disability turns to alternate option for communication usually hand signs. But a lot of times people who speak do not understand hand signs because the are used to speaking most at times. So in order for these two categories to communicate there needs to be a solution to bridge this gap. This is important in order to foster and improve communication between people who communicate with hand signs and people who speak.. eg parents and their kids. 


## How is it used?

The solution will be a mobile application used both by the person communicating using hand signs and the person speaking. So, In a situation for communication the person making hand signs can video/sound record the person speaking and after the person is done speaking , the app interpretes what the person has said into hand signs they can understand.... on the other hand also the person who speaks video records the person performing hand signs and afterwards the app interpretes this hand signs to speech or text that the person speaking can understand.


## Data sources and AI methods
Where does your data come from? Do you collect it yourself or do you use data collected by someone else?
The data for this project will come from external sources, and already existing data set on handsigns and natural language, such as data from Kaggle eg :
[hand signs kaggle](https://www.kaggle.com/datasets/jeyasrisenthil/hand-signs-asl-hand-sign-data)


## Challenges

What does your project _not_ solve? Which limitations and ethical considerations should be taken into account when deploying a solution like this?
the major limitation for this is that real time interpretation will be very difficult since the interpretation to and from hand signs to text has to be real time considering the speed,intonation, accents etc of how a person is communicating.

## What next?

For this project to take of, it will need engineers well vast in machine learning and AI, most specifically in computer vision and NLP

